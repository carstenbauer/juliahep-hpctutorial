{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a49e61-34c5-46c1-9f63-044aedbce8ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# JuliaHEP 2023 Workshop -  HPC Tutorial\n",
    "\n",
    "**When:** November 7, 2023\n",
    "\n",
    "**Where:** Erlangen Centre for Astroparticle Physics (ECAP)\n",
    "\n",
    "**GitHub repository:** https://github.com/carstenbauer/juliahep-hpctutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a90034-cd8a-4020-812c-8946947bfbbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What's the plan for this tutorial?\n",
    "\n",
    "* Use Julia on an HPC cluster (maybe for the first time?)\n",
    "* Study the node-level performance scaling of a simple computational kernel\n",
    "* Learn about thread pinning, NUMA, and how to control both from within Julia\n",
    "* (If time permits: move the computation to an NVIDIA A100 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ba331-af39-4ddd-b91e-2a6e93e8e4f8",
   "metadata": {},
   "source": [
    "## Julia interactively on HPC clusters. How?\n",
    "\n",
    "* Terminal approach (SSH + e.g. vim + REPL)\n",
    "* **VS Code** → Remote SSH Extension\n",
    "  * login node (easy)\n",
    "  * compute node (tricky, sometimes impossible): [PC2 docs](https://upb-pc2.atlassian.net/wiki/spaces/PC2DOK/pages/1902225/Access+for+Applications+like+Visual+Studio+Code#Compute-nodes) and/or README.md\n",
    "\n",
    "**For this tutorial, we'll use the PC2 JupyterHub for simplicity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de30091-879b-452a-9bc0-d38b3618f80a",
   "metadata": {},
   "source": [
    "### [PC2 JupyterHub](https://jh.pc2.uni-paderborn.de/)\n",
    "\n",
    "**Link:** https://pc2.de/go/jupyterhub\n",
    "\n",
    "Most participants have access to the [Noctua 2](https://pc2.uni-paderborn.de/hpc-services/available-systems/noctua2) cluster through the [PC2 JupyterHub](https://jh.pc2.uni-paderborn.de/hub/home).\n",
    "\n",
    "In this case, **a browser is all that's needed!**\n",
    "\n",
    "#### Getting started\n",
    "\n",
    "* Login to [PC2 JupyterHub](https://pc2.de/go/jupyterhub) with the provided credentials.\n",
    "* After login, click on the \"Start Server\" button.\n",
    "* Select the **\"JuliaHEP - HPC Tutorial (full CPU node)\"** preset (should already be the default) and click on \"Start\". This will start a Jupyter server on a Noctua 2 compute node (might take a little while).\n",
    "* Once in Jupyter, you should see a folder with your username in the left side bar. Navigate into this folder. In it you'll find a local copy of this git repository that you can use for the tutorial.\n",
    "* To make Julia (and the IJulia kernel) available, click on the little blue hexagon in the side bar on the left. Then, type \"jupyter\" into the search bar at the top. Hover over `JupyterKernel-Julia/1.9.3-foss-2022a-CUDA-11.7.0` and click on the appearing \"Load\" button.\n",
    "* You should be all set up! Feel free to open the first notebook `1_axpy_cpu.ipynb` and, in the top right corner, select the kernel **\"Julia (8 threads) 1.9.3\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e0fea-0553-4781-a5d9-c303be93e5ba",
   "metadata": {},
   "source": [
    "## Computational kernel: AXPY\n",
    "\n",
    "\"*A time X plus Y*\"\n",
    "\n",
    "$$ \\vec{y} = a \\cdot \\vec{x} + \\vec{y} $$\n",
    "\n",
    "Depending on the data type / precision:\n",
    "\n",
    "* **S**AXPY (S = single precision, i.e. `Float32`)\n",
    "* **D**AXPY (D = double precision, i.e. `Float64`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e6250-1c1d-4d37-9452-85cdbdc6ad20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function axpy_serial!(y, a, x)\n",
    "    #\n",
    "    # TODO: Implement the (serial) AXPY kernel.\n",
    "    #\n",
    "    for i in eachindex(x,y)\n",
    "        @inbounds y[i] = a * x[i] + y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91121fe4-ceea-4ae9-a6ef-8f02459a518f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why AXPY?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379e1c8-2f55-4c02-a48e-13b0d32e762c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What limits performance of computations?\n",
    "* memory access speed (*memory-bound*)\n",
    "* how fast floating-point operations (flops) can be done (*compute-bound*)\n",
    "\n",
    "The performance of most scientific codes is **memory-bound** these days!\n",
    "\n",
    "**CPU (AMD EPYC 7763)**\n",
    "\n",
    "*Peak compute performance over peak memory bandwidth*\n",
    "\n",
    "$$\n",
    "\\dfrac{3.5 \\ [\\textrm{TFlop/s}]}{200 \\ [\\textrm{GB/s}]} \\cdot 8 \\ \\textrm{B} = 140\n",
    "$$\n",
    "\n",
    "140 flops per number read, i.e. 8 bytes for `Float64`\n",
    "\n",
    "**GPU (NVIDIA A100)**\n",
    "\n",
    "*Peak compute performance over peak memory bandwidth* (only using CUDA cores)\n",
    "\n",
    "$$\n",
    "\\dfrac{19.5 \\ [\\textrm{TFlop/s}]}{1.5 \\ [\\textrm{TB/s}]} \\cdot 4 \\ \\textrm{B} = 52\n",
    "$$\n",
    "\n",
    "52 flops per number read, i.e. 4 bytes for `Float32`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44467e3-3980-4f63-ad79-c2c5977e5cea",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "* How many **bytes** are transferred per iteration in AXPY?\n",
    "* How many **flops** (floating point operations) are performed per iteration in AXPY?\n",
    "* Is AXPY compute- or memory-bound?\n",
    "\n",
    "**\"Trick\" question:** How many **bytes** would be transferred in a non-inplace variant, i.e. `z[i] = a * x[i] + y[i]`? (Hint: It's likely not what you think 😉)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2708c09-593a-4653-8b6f-d710e97b7d69",
   "metadata": {},
   "source": [
    "Let's benchmark the performance of our AXPY kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ae398-51ef-4b81-804c-318f61483d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "const N = 2^30\n",
    "\n",
    "a = 3.141\n",
    "x = rand(N)\n",
    "y = rand(N)\n",
    "\n",
    "@btime axpy_serial!($y, $a, $x) samples=5 evals=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376064b2-c12e-499d-9124-4f20b6447169",
   "metadata": {},
   "source": [
    "Is this fast? What should we compare it to?\n",
    "\n",
    "Let's look at the **memory bandwidth** (data transfer to/from memory per unit time) and the compute performance (flops per unit time) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c213c4e-910c-4c31-9d47-9076f277d36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "function generate_input_data(; N, dtype, kwargs...)\n",
    "    a = dtype(3.141)\n",
    "    x = rand(dtype, N)\n",
    "    y = rand(dtype, N)\n",
    "    return a,x,y\n",
    "end\n",
    "\n",
    "function measure_perf(f::F; N=2^30, dtype=Float64, verbose=true, kwargs...) where {F}  \n",
    "    # input data\n",
    "    a,x,y = generate_input_data(; N, dtype, kwargs...)\n",
    "\n",
    "    # time measurement\n",
    "    t = @belapsed $f($y, $a, $x) evals = 2 samples = 10\n",
    "    \n",
    "    # compute memory bandwidth and flops\n",
    "    bytes = 3 * sizeof(dtype) * N # TODO: num bytes transferred in AXPY kernel (all iterations)\n",
    "    flops = 2 * N # TODO: num flops performed in AXPY kernel (all iterations)\n",
    "    mem_rate = bytes * 1e-9 / t # TODO: memory bandwidth in GB/s\n",
    "    flop_rate = flops * 1e-9 / t # TODO: flops in GFLOP/s\n",
    "    \n",
    "    if verbose\n",
    "        println(\"Dtype: $dtype\")\n",
    "        println(\"\\tMemory Bandwidth (GB/s): \", round(mem_rate; digits=2))\n",
    "        println(\"\\tCompute (GFLOP/s): \", round(flop_rate; digits=2))\n",
    "    end\n",
    "    return mem_rate, flop_rate\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6472f-a1fb-4919-8ed9-85f4dc38bbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "measure_perf(axpy_serial!);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab68d0-cb95-4f5d-bdcc-a5b5fd4c0e81",
   "metadata": {},
   "source": [
    "This is about 20% of the theoretical value for one entire CPU (with 64 cores). This will serve as our single-core performance reference point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafde1a-3aeb-4ee6-8ad2-90b2c91c373b",
   "metadata": {},
   "source": [
    "## Node-level parallelisation (multithreading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf1e77-5f6a-4196-b195-70ce1dd6a0e5",
   "metadata": {},
   "source": [
    "**SIMD:** `axpy_serial!` is already *parallel* at instruction level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06efab1-08fd-4d67-9500-82665b382214",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none axpy_serial!(y,a,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cfad5f-2866-4470-8bc2-d229578a8e69",
   "metadata": {},
   "source": [
    "We want to parallelize our AXPY kernel via **multithreading**.\n",
    "\n",
    "Julia provides the `@threads` macro to multithread for-loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab469153-3ecd-4439-a95e-fdaa6104efc8",
   "metadata": {},
   "source": [
    "**Make sure that you actually have multiple threads in this Julia session!** (I recommend 8 threads on Noctua 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035847d9-ecbb-437b-b450-5615902d6f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Base.Threads: @threads, nthreads\n",
    "\n",
    "@assert nthreads() > 1\n",
    "nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a8b1d-bb29-4286-b9bf-86282351a32c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function axpy_multithreading_dynamic!(y, a, x)\n",
    "    #\n",
    "    # TODO: Implement a naive multithreaded AXPY kernel (with @threads).\n",
    "    #\n",
    "    @threads for i in eachindex(x,y)\n",
    "        @inbounds y[i] = a * x[i] + y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517406a9-2881-420d-8423-d83f18929e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "measure_perf(axpy_multithreading_dynamic!);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105b601-3d3d-4f95-ad38-a5d746616e02",
   "metadata": {},
   "source": [
    "🙁 **What's going on?! Why no (or not much) speedup?!** 😢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9876df9-499b-4955-9083-966f9acf1f3e",
   "metadata": {},
   "source": [
    "### Pinning Julia threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292bd15-b73a-40d8-ad86-9da6980bc070",
   "metadata": {},
   "source": [
    "**Why** pin threads?\n",
    "\n",
    "* stable performance (e.g. avoid fluctuations in benchmarks)\n",
    "* avoid double occupation of CPU-cores / CPU-threads\n",
    "* fixed memory locality\n",
    "* (hardware performance monitoring → [LIKWID.jl](https://github.com/JuliaPerf/LIKWID.jl))\n",
    "\n",
    "**How** pin Julia threads? → [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl)\n",
    "\n",
    "What about external tools like `numactl`, `taskset`, etc.? Doesn't work reliably because they often [can't distinguish](https://discourse.julialang.org/t/thread-affinitization-pinning-julia-threads-to-cores/58069/5) between Julia threads and other internal threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929f363-fcb5-4f5e-8102-09f28a240c71",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"./imgs/threadpinning_pinthreads.svg\" width=700>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3df38-3153-492b-8dbe-b9ab4cc9161e",
   "metadata": {},
   "source": [
    "(More? See my short talk at JuliaCon2023 @ MIT: https://youtu.be/6Whc9XtlCC0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02505215-cc90-4512-aa73-fae51142c90c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ThreadPinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddd4b0-d652-4ba6-b907-fc6a5ae7b97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7b27b-d7b5-4585-b669-6e08c8251f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinthreads(:cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba5ff7-18d8-4f89-af2c-e7051fb47869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b369b4-a623-4366-a30c-495bdbc02483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinthreads(:sockets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e6949-9b3a-4f1e-8ce1-091c665c8046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056ff7a-0847-4935-a2e8-48aac58e9041",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Benchmark with pinned threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c5f71-3533-4c2a-b9ad-1be6062c613c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinthreads(:cores)\n",
    "measure_perf(axpy_multithreading_dynamic!);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a9a5c-f254-4bf2-88ee-98b74c9f0843",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Still the same performance?!** 😢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5f3e8-a8e1-4826-a5a4-2b2e3d97d8ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data placement (NUMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4051db7-86fa-466a-b7c0-bf970dc7d3d6",
   "metadata": {},
   "source": [
    "NUMA = **n**on-**u**niform **m**emory **a**ccess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb62f86-bd7c-4857-b68c-9f4b2e0fc3fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "One (of two) AMD Milan CPUs in Noctua 2:\n",
    "\n",
    "<img src=\"./imgs/amd_milan_cpu_die.svg\" width=800>\n",
    "\n",
    "**Image source:** AMD, [High Performance Computing (HPC) Tuning Guide for AMD EPYCTM 7003 Series Processors](https://www.amd.com/system/files/documents/high-performance-computing-tuning-guide-amd-epyc7003-series-processors.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571f3cc-0b4d-40cf-8219-0e2e5ec2c9ec",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/noctua2_topo.svg\" width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be4f12-d6b7-486a-89e7-e7c79119c3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo(; groupby=:numa) # switch from socket/CPU grouping to NUMA grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bbe48-2215-4cf5-918b-0bb85bf8e272",
   "metadata": {},
   "source": [
    "#### How to control data placement (explicitly)?\n",
    "→ [NUMA.jl](https://github.com/JuliaPerf/NUMA.jl)\n",
    "\n",
    "`Vector{Float64}(numanode(i), length)` (kind of similar to `Vector{Float64}(undef, length)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01947988-184e-4a60-8b28-f7416f6df898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using NUMA, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2202143-08d1-4893-b01d-83809d27766f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = Vector{Float64}(numanode(1), 100); rand!(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71420185-5321-406d-9326-2ce7a430bc48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "which_numa_node(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c548c1e-9144-42cd-9c72-9377f682cb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = Vector{Float64}(numanode(8), 100); rand!(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346e6a5-a839-466b-a7cf-c4c43f6fe1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "which_numa_node(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53ca44-88cc-41c7-973a-77b809edb750",
   "metadata": {},
   "source": [
    "Let's do a quick and dirty benchmark to get an idea how much this matters for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3966b39-24a6-43d4-9913-e57a20d00080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node1 = current_numa_node()\n",
    "node2 = mod1(current_numa_node() + nnumanodes()÷2, nnumanodes()) # numa node in other CPU/socket\n",
    "\n",
    "println(\"local NUMA node\")\n",
    "x = Vector{Float64}(numanode(node1), N); rand!(x)\n",
    "y = Vector{Float64}(numanode(node1), N); rand!(y)\n",
    "\n",
    "@btime axpy_serial!($y, $a, $x) samples=5 evals=3;\n",
    "\n",
    "println(\"distant NUMA node\")\n",
    "x = Vector{Float64}(numanode(node2), N); rand!(x)\n",
    "y = Vector{Float64}(numanode(node2), N); rand!(y)\n",
    "\n",
    "@btime axpy_serial!($y, $a, $x) samples=5 evals=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b979ec-9350-4783-9f22-fc0bc82800a8",
   "metadata": {},
   "source": [
    "Note that the performance issue will be mouch more pronounced in multithreaded cases, where different threads might try to access the same non-local data over the same memory channel(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2a357-10ad-4503-8803-4d651beb3767",
   "metadata": {},
   "source": [
    "#### How to control data placement (implicitly)?\n",
    "\n",
    "→ **\"First-touch\" policy**\n",
    "\n",
    "```julia\n",
    "x = Vector{Float64}(undef, 10)   # allocation, no \"touch\" yet\n",
    "rand!(x)                         # first touch == first write\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d9677-0d97-44db-b693-c71b68201a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinthreads(:numa)\n",
    "threadinfo(; groupby=:numa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473431a9-014b-4c25-ba2c-f3df9bc481cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tid in 1:8\n",
    "    @sync @tspawnat tid begin            # ThreadPinning.@tspawnat creates *sticky* tasks that don't migrate between threads\n",
    "        x = Vector{Float64}(undef, 10)   # allocation, no \"touch\" yet\n",
    "        rand!(x)                         # first touch\n",
    "        @show tid, which_numa_node(x)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4344f-a0a8-49bc-8dec-f5c36bda7d5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### NUMA-optimized AXPY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12311c-7624-4a10-b6a9-7c0949bf8a08",
   "metadata": {},
   "source": [
    "**Question**\n",
    "* How can we modify our AXPY benchmark to optimize for local memory accesses (based on the first-touch policy)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfaac5-3365-4686-b9de-002ed1107130",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "function generate_input_data(; N, dtype, parallel=false, kwargs...)\n",
    "    #\n",
    "    # TODO: introduce a new keyword argument that, when set to true, initializes the data in parallel\n",
    "    #       (in the same way as we'll later use it)\n",
    "    #\n",
    "    a = dtype(3.141)\n",
    "    x = Vector{dtype}(undef, N)\n",
    "    y = Vector{dtype}(undef, N)\n",
    "    if !parallel\n",
    "        rand!(x)\n",
    "        rand!(y)\n",
    "    else\n",
    "        @threads for i in eachindex(x,y)\n",
    "            x[i] = rand()\n",
    "            y[i] = rand()\n",
    "        end\n",
    "    end\n",
    "    return a,x,y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff6e8b-78a9-4273-a159-f5bf0d040fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinthreads(:numa)\n",
    "measure_perf(axpy_multithreading_dynamic!; parallel=false);\n",
    "measure_perf(axpy_multithreading_dynamic!; parallel=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1551b9-ab59-4211-952c-f7ed23e521cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Speedup! Yeah!** 😄 🎉\n",
    "\n",
    "But.... less than expected!? 😕\n",
    "\n",
    "**Question**\n",
    "* What kind of speedup would we expect (ideally)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d7999-7bc7-4a4c-bf19-6ccdb7e5d792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo(; groupby=:numa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a617a62-41b4-40d1-80cd-450b343949d1",
   "metadata": {},
   "source": [
    "### Tasks vs Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8efdb-3410-4856-a782-f313e6d556e3",
   "metadata": {},
   "source": [
    "Conceptually, Julia implements **task-based multithreading**.\n",
    "\n",
    "**A user shouldn't care about threads but tasks!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df01883-7563-4e5e-8fc5-ec23354f3624",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./imgs/julia_tasks_vs_threads.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ad1fb-7c0d-48a7-93b4-ec89e842bb13",
   "metadata": {},
   "source": [
    "In **\"traditional\" HPC**, we typically care about threads directly, i.e. we tell every thread what it should do.\n",
    "\n",
    "In Julia's **task-based multithreading**, a task - e.g. a computational piece of a code - is only marked for **parallel execution** (`@spawn`, `@threads`) on **any** of the available Julia threads. Julias **dynamic scheduler** will then take care of running the task on any of the threads (the task might even migrate!).\n",
    "\n",
    "*Advantages:*\n",
    "* high-level abstraction\n",
    "* **composability / nestability** (Multithreaded code can call multithreaded code can call multithreaded code ....)\n",
    "\n",
    "*Disadvantages:*\n",
    "* potential scheduling overhead\n",
    "* **task → thread assignment uncertain (can vary dynamically + task migration)**\n",
    "* can get in the way when performance engineering\n",
    "  * scheduler has limited information (e.g. about the system topology)\n",
    "  * low-level profiling (e.g. with LIKWID) requires fixed `task → thread → core` mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd1b1d-7e55-43f0-b2d1-6a4b645a924a",
   "metadata": {},
   "source": [
    "#### Opt-out of dynamic scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf790a5-cc2b-4c59-969a-79c317ea14d8",
   "metadata": {},
   "source": [
    "We can pt-out of Julia's dynamic scheduling and get **guarantees about the task-thread assignment** (and the iterations → task mapping).\n",
    "\n",
    "Syntax: `@threads :static for ...`\n",
    "\n",
    " * splits up the iteration space into `nthreads()` even, contiguous blocks (in-order) and creates precisely one task per block\n",
    " * **statically** maps tasks to threads, specifically: task 1 -> thread 1, task 2 -> thread 2, etc.\n",
    "   * no task migration, i.e. **fixed task-thread mapping** 👍\n",
    "   * only little overhead 👍\n",
    "   * not composable / nestable 👎\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e326d-a11a-4563-94a7-350ff860dc74",
   "metadata": {},
   "source": [
    "**In short:**\n",
    "\n",
    "Dynamic scheduling: `@spawn`, `@threads :dynamic` (default)\n",
    "\n",
    "Static scheduling (i.e. fixed task → thread mapping): `ThreadPinning.@tspawnat`, `@threads :static`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a891a2a-7ad0-4316-a382-4c64e1e27316",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Statically scheduled AXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe245af-82a0-43bc-9b05-535e20a41429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function axpy_multithreading_static!(y, a, x)\n",
    "    #\n",
    "    # TODO: Implement a statically scheduled multithreaded AXPY kernel (with @threads :static).\n",
    "    #\n",
    "    @threads :static for i in eachindex(x,y)\n",
    "        @inbounds y[i] = a * x[i] + y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe53c6-62d9-499a-8b49-357b7b6b151b",
   "metadata": {},
   "source": [
    "We also need to adapt the input data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b19b64-47bf-4085-b379-0f1dbef68f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function generate_input_data(; N, dtype, parallel=false, static=false, kwargs...)\n",
    "    #\n",
    "    # TODO: introduce a new keyword argument `static` that, when set to true, initializes the data in parallel with static scheduling\n",
    "    #       (in the same way as we'll later use it)\n",
    "    #\n",
    "    a = dtype(3.141)\n",
    "    x = Vector{dtype}(undef, N)\n",
    "    y = Vector{dtype}(undef, N)\n",
    "    if !parallel\n",
    "        rand!(x)\n",
    "        rand!(y)\n",
    "    else\n",
    "        if !static\n",
    "            @threads for i in eachindex(x,y)\n",
    "                x[i] = rand()\n",
    "                y[i] = rand()\n",
    "            end\n",
    "        else\n",
    "            @threads :static for i in eachindex(x,y)\n",
    "                x[i] = rand()\n",
    "                y[i] = rand()\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return a,x,y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d40c74-582e-402d-a8c5-aef368eda4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinthreads(:numa)\n",
    "measure_perf(axpy_multithreading_static!; parallel=false, static=true);\n",
    "measure_perf(axpy_multithreading_static!; parallel=true, static=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83fddac-da2b-4406-b3e2-fa6edd11fd37",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Finally, we're in the ballpark of the expected speedup!** 😄 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (8 threads) 1.9.3",
   "language": "julia",
   "name": "julia-_8-threads_-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
