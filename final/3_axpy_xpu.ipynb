{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backend-agnostic AXPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #AA0000\"><b>Note:</b> For the GPU part in this notebook, you'll have to make sure to be on a Noctua 2 GPU node!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl) to write backend agnostic code for XPU.\n",
    "\n",
    "(\"**X**PU\" = **C**PU, **G**PU, ...)\n",
    "\n",
    "**Backends**\n",
    "* `CPU()`\n",
    "* `CPU(; static=true)`\n",
    "* `CUDABackend()`\n",
    "* ...\n",
    "\n",
    "Note that functions like\n",
    "\n",
    "* `KernelAbstractions.zeros(backend, dtype, N)`\n",
    "\n",
    "automatically **initialize in parallel** under the hood!\n",
    "\n",
    "The following AXPY variant is generic and runs on CPUs and GPUs (by NVIDIA, AMD, Intel, ...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using KernelAbstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "axpy_ka! (generic function with 4 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kernel function axpy_ka!(y, a, @Const(x))\n",
    "    I = @index(Global)\n",
    "    @inbounds y[I] = a * x[I] + y[I]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measure_perf (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "using Random\n",
    "\n",
    "function generate_input_data(backend::Backend; N, dtype, kwargs...)\n",
    "    a = dtype(3.141)\n",
    "    x = rand!(KernelAbstractions.zeros(backend, dtype, N))\n",
    "    y = rand!(KernelAbstractions.zeros(backend, dtype, N))\n",
    "    return a,x,y\n",
    "end\n",
    "\n",
    "function measure_perf(backend::Backend; N=2^27, dtype=Float64, verbose=true, kwargs...)  \n",
    "    # input data\n",
    "    a,x,y = generate_input_data(backend; N, dtype, kwargs...)\n",
    "    \n",
    "    # time measurement\n",
    "    t = @belapsed begin\n",
    "        kernel = axpy_ka!($backend, 1024, $(size(y)))\n",
    "        kernel($y, $a, $x, ndrange = $(size(y)))\n",
    "        KernelAbstractions.synchronize($backend)\n",
    "    end evals=2 samples=10\n",
    "    \n",
    "    # compute memory bandwidth and flops\n",
    "    bytes = 3 * sizeof(dtype) * N\n",
    "    flops = 2 * N\n",
    "    mem_rate = bytes * 1e-9 / t\n",
    "    flop_rate = flops * 1e-9 / t\n",
    "    \n",
    "    if verbose\n",
    "        println(\"Dtype: $dtype\")\n",
    "        println(\"\\tMemory Bandwidth (GB/s): \", round(mem_rate; digits=2))\n",
    "        println(\"\\tCompute (GFLOP/s): \", round(flop_rate; digits=2))\n",
    "    end\n",
    "    return mem_rate, flop_rate\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype: Float64\n",
      "\tMemory Bandwidth (GB/s): 124.47\n",
      "\tCompute (GFLOP/s): 10.37\n",
      "Dtype: Float64\n",
      "\tMemory Bandwidth (GB/s): 377.58\n",
      "\tCompute (GFLOP/s): 31.47\n"
     ]
    }
   ],
   "source": [
    "using ThreadPinning\n",
    "pinthreads(:numa)\n",
    "\n",
    "measure_perf(CPU());\n",
    "measure_perf(CPU(; static=true));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA A100-SXM4-40GB"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "CUDA.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype: Float64\n",
      "\tMemory Bandwidth (GB/s): 1328.38\n",
      "\tCompute (GFLOP/s): 110.7\n"
     ]
    }
   ],
   "source": [
    "measure_perf(CUDABackend());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison: `CUBLAS.axpy!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measure_perf_cublas (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "using BenchmarkTools\n",
    "\n",
    "\"Computes AXPY using the built-in function `CUBLAS.axpy!` provided by NVIDIA\"\n",
    "function axpy_cublas!(y, a, x)\n",
    "    CUDA.@sync CUBLAS.axpy!(length(x), a, x, y)\n",
    "end\n",
    "\n",
    "function measure_perf_cublas(; N=2^27, dtype=Float64, verbose=true, kwargs...)  \n",
    "    # input data\n",
    "    a = dtype(3.141)\n",
    "    x = CUDA.rand(dtype, N)\n",
    "    y = CUDA.rand(dtype, N)\n",
    "    \n",
    "    # time measurement\n",
    "    t = @belapsed axpy_cublas!($y, $a, $x) evals=2 samples=10\n",
    "    \n",
    "    # compute memory bandwidth and flops\n",
    "    bytes = 3 * sizeof(dtype) * N\n",
    "    flops = 2 * N\n",
    "    mem_rate = bytes * 1e-9 / t\n",
    "    flop_rate = flops * 1e-9 / t\n",
    "    \n",
    "    if verbose\n",
    "        println(\"Dtype: $dtype\")\n",
    "        println(\"\\tMemory Bandwidth (GB/s): \", round(mem_rate; digits=2))\n",
    "        println(\"\\tCompute (GFLOP/s): \", round(flop_rate; digits=2))\n",
    "    end\n",
    "    return mem_rate, flop_rate\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype: Float64\n",
      "\tMemory Bandwidth (GB/s): 1333.37\n",
      "\tCompute (GFLOP/s): 111.11\n"
     ]
    }
   ],
   "source": [
    "measure_perf_cublas();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Broadcasting, CUDA kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA\n",
    "\n",
    "\"Computes the AXPY via broadcasting\"\n",
    "function axpy_broadcast!(y, a, x)\n",
    "    CUDA.@sync y .= a .* x .+ y\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\"CUDA kernel for computing AXPY on the GPU\"\n",
    "function _axpy_cudakernel!(y, a, x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(y)\n",
    "        @inbounds y[i] = a * x[i] + y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\"Computes AXPY on the GPU using the custom CUDA kernel `_axpy_cudakernel!` above\"\n",
    "function axpy_cuda!(a, x, y; nthreads, nblocks)\n",
    "    CUDA.@sync @cuda(threads=nthreads,\n",
    "                     blocks=nblocks,\n",
    "                     _axpy_cudakernel!(a, x, y))\n",
    "    return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark all variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────┬─────────┬─────────┬───────────┐\n",
      "│\u001b[1m            Variant \u001b[0m│\u001b[1m Runtime \u001b[0m│\u001b[1m   FLOPS \u001b[0m│\u001b[1m Bandwidth \u001b[0m│\n",
      "│\u001b[90m                    \u001b[0m│\u001b[90m      ms \u001b[0m│\u001b[90m GFLOP/s \u001b[0m│\u001b[90m      GB/s \u001b[0m│\n",
      "├────────────────────┼─────────┼─────────┼───────────┤\n",
      "│          Broadcast │ 5.01724 │ 204.096 │   1224.58 │\n",
      "│        CUDA kernel │ 4.60986 │ 222.133 │    1332.8 │\n",
      "│             CUBLAS │ 4.59248 │ 222.973 │   1337.84 │\n",
      "│ KernelAbstractions │ 4.62194 │ 221.552 │   1329.31 │\n",
      "└────────────────────┴─────────┴─────────┴───────────┘\n",
      "Theoretical Memory Bandwidth of NVIDIA A100: 1555 GB/s\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "using PrettyTables\n",
    "\n",
    "axpy_flops(t; len) = 2.0 * len * 1e-9 / t # GFLOP/s\n",
    "axpy_bandwidth(t; dtype, len) = 3.0 * sizeof(dtype) * len * 1e-9 / t # GB/s\n",
    "\n",
    "function axpy_gpu_bench_all()\n",
    "    dtype = Float32\n",
    "    nthreads = 1024\n",
    "    nblocks = 500_000\n",
    "    len = nthreads * nblocks # vector length\n",
    "    a = convert(dtype, 3.1415)\n",
    "    xgpu = CUDA.ones(dtype, len)\n",
    "    ygpu = CUDA.ones(dtype, len)\n",
    "\n",
    "    t_broadcast_gpu = @belapsed axpy_broadcast!($ygpu, $a, $xgpu) samples=10 evals=2\n",
    "    t_cuda_kernel = @belapsed axpy_cuda!($ygpu, $a, $xgpu; nthreads = $nthreads,\n",
    "                                                 nblocks = $nblocks) samples=10 evals=2\n",
    "    t_cublas = @belapsed axpy_cublas!($ygpu, $a, $xgpu) samples=10 evals=2\n",
    "    \n",
    "    backend = CUDABackend()\n",
    "    t_ka = @belapsed begin\n",
    "        kernel = axpy_ka!($backend, $nthreads, $(size(ygpu)))\n",
    "        kernel($ygpu, $a, $xgpu, ndrange = $(size(ygpu)))\n",
    "        KernelAbstractions.synchronize($backend)\n",
    "    end evals=2 samples=10\n",
    "    \n",
    "    times = [t_broadcast_gpu, t_cuda_kernel, t_cublas, t_ka]\n",
    "\n",
    "    flops = axpy_flops.(times; len)\n",
    "    bandwidths = axpy_bandwidth.(times; dtype, len)\n",
    "\n",
    "    labels = [\"Broadcast\", \"CUDA kernel\", \"CUBLAS\", \"KernelAbstractions\"]\n",
    "    data = hcat(labels, 1e3 .* times, flops, bandwidths)\n",
    "    pretty_table(data;\n",
    "                 header = ([\"Variant\", \"Runtime\", \"FLOPS\", \"Bandwidth\"],\n",
    "                           [\"\", \"ms\", \"GFLOP/s\", \"GB/s\"]))\n",
    "    println(\"Theoretical Memory Bandwidth of NVIDIA A100: 1555 GB/s\")\n",
    "    return nothing\n",
    "end;\n",
    "\n",
    "axpy_gpu_bench_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (128 threads) 1.9.3",
   "language": "julia",
   "name": "julia-_128-threads_-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
